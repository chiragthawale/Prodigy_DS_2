# -*- coding: utf-8 -*-
"""Prodigy_DS_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YZ1zQykYuJsSI_RsHcstR60sVSRrU1W4
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import Image, display
# %matplotlib inline

#load the train dataset
train = pd.read_csv("/content/Untitled Folder/train.csv")

#inspect the first few rows of the train dataset
display(train.head())

# set the index to passengerId
train = train.set_index('PassengerId')

#load the test dataset
test = pd.read_csv("/content/Untitled Folder/test.csv")

#inspect the first few rows of the test dataset
display(test.head())

#by calling the shape attribute of the train dataset we can observe that there are 891 observations and 11 columns
#in the data set
train.shape

# Check out the data summary
# Age, Cabin and Embarked has missing data
train.head()

# identify datatypes of the 11 columns, add the stats to the datadict
datadict = pd.DataFrame(train.dtypes)
datadict

# identify missing values of the 11 columns,add the stats to the datadict
datadict['MissingVal'] = train.isnull().sum()
datadict

# Identify number of unique values, For object nunique will the number of levels
# Add the stats the data dict
datadict['NUnique']=train.nunique()
datadict

# Identify the count for each variable, add the stats to datadict
datadict['Count']=train.count()
datadict

# rename the 0 column
datadict = datadict.rename(columns={0:'DataType'})
datadict

# get discripte statistcs on "object" datatypes
train.describe(include=['object'])

# get discriptive statistcs on "number" datatypes
train.describe(include=['number'])

train.Survived.value_counts(normalize=True)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load your dataset, adjust the file path accordingly
train = pd.read_csv("/content/Untitled Folder/train.csv")

# Assuming 'train' contains the necessary columns for your analysis

# Now you can use 'train' in your plotting code
fig, axes = plt.subplots(2, 4, figsize=(16, 10))
sns.countplot(x='Survived', data=train, ax=axes[0, 0])
sns.countplot(x='Pclass', data=train, ax=axes[0, 1])
sns.countplot(x='Sex', data=train, ax=axes[0, 2])
sns.countplot(x='SibSp', data=train, ax=axes[0, 3])
sns.countplot(x='Parch', data=train, ax=axes[1, 0])
sns.countplot(x='Embarked', data=train, ax=axes[1, 1])
sns.histplot(train['Fare'], kde=True,ax=axes[1,2])
sns.histplot(train['Age'].dropna(),kde=True,ax=axes[1,3])

# Show the plots
plt.show()

figbi, axesbi = plt.subplots(2, 4, figsize=(16, 10))
train.groupby('Pclass')['Survived'].mean().plot(kind='barh',ax=axesbi[0,0],xlim=[0,1])
train.groupby('SibSp')['Survived'].mean().plot(kind='barh',ax=axesbi[0,1],xlim=[0,1])
train.groupby('Parch')['Survived'].mean().plot(kind='barh',ax=axesbi[0,2],xlim=[0,1])
train.groupby('Sex')['Survived'].mean().plot(kind='barh',ax=axesbi[0,3],xlim=[0,1])
train.groupby('Embarked')['Survived'].mean().plot(kind='barh',ax=axesbi[1,0],xlim=[0,1])
sns.boxplot(x="Survived", y="Age", data=train,ax=axesbi[1,1])
sns.boxplot(x="Survived", y="Fare", data=train,ax=axesbi[1,2])

sns.jointplot(x="Age", y="Fare", data=train);

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np  # Import NumPy

# Load your dataset, adjust the file path accordingly
train = pd.read_csv("/content/Untitled Folder/train.csv")

# Assuming you want to include only numeric columns in the correlation analysis
numeric_columns = train.select_dtypes(include=['float64', 'int64'])

f, ax = plt.subplots(figsize=(10, 8))
corr = numeric_columns.corr()

# Use bool directly instead of np.bool
mask = np.zeros_like(corr, dtype=bool)

sns.heatmap(corr,
            mask=mask,
            cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)
plt.show()

train['Name_len']=train.Name.str.len()

train['Ticket_First']=train.Ticket.str[0]

train['FamilyCount']=train.SibSp+train.Parch

train['Cabin_First']=train.Cabin.str[0]

# Regular expression to get the title of the Name
train['title'] = train.Name.str.extract('\, ([A-Z][^ ]*\.)',expand=False)

train.title.value_counts().reset_index()

# we see that there are 15 Zero values and its reasonbale
# to flag them as missing values since every ticket
# should have a value greater than 0
print((train.Fare == 0).sum())

# mark zero values as missing or NaN
train.Fare = train.Fare.replace(0, np.NaN)

# validate to see if there are no more zero values
print((train.Fare == 0).sum())

# keep the index
train[train.Fare.isnull()].index

train.Fare.mean()

# impute the missing Fare values with the mean Fare value
train.Fare.fillna(train.Fare.mean(),inplace=True)

# validate if any null values are present after the imputation
train[train.Fare.isnull()]

# we see that there are 0 Zero values
print((train.Age == 0).sum())

# impute the missing Age values with the mean Fare value
train.Age.fillna(train.Age.mean(),inplace=True)

# validate if any null values are present after the imputation
train[train.Age.isnull()]

# We see that a majority 77% of the Cabin variable has missing values.
# Hence will drop the column from training a machine learnign algorithem
train.Cabin.isnull().mean()

train.info()

train.columns

trainML = train[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',
       'Fare', 'Embarked', 'Name_len', 'Ticket_First', 'FamilyCount',
       'title']]

# drop rows of missing values
trainML = trainML.dropna()

# check the datafram has any missing values
trainML.isnull().sum()

# Import Estimator AND Instantiate estimator class to create an estimator object
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

X_Age = trainML[['Age']].values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_Age,y)
# Make a prediction
y_predict = lr.predict(X_Age)
y_predict[:10]
(y == y_predict).mean()

X_Fare = trainML[['Fare']].values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_Fare,y)
# Make a prediction
y_predict = lr.predict(X_Fare)
y_predict[:10]
(y == y_predict).mean()

X_sex = pd.get_dummies(trainML['Sex']).values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_sex, y)
# Make a prediction
y_predict = lr.predict(X_sex)
y_predict[:10]
(y == y_predict).mean()

X_pclass = pd.get_dummies(trainML['Pclass']).values
y = trainML['Survived'].values
lr = LogisticRegression()
lr.fit(X_pclass, y)
# Make a prediction
y_predict = lr.predict(X_pclass)
y_predict[:10]
(y == y_predict).mean()

from sklearn.ensemble import RandomForestClassifier
X=trainML[['Age', 'SibSp', 'Parch',
       'Fare', 'Name_len', 'FamilyCount']].values # Taking all the numerical values
y = trainML['Survived'].values
RF = RandomForestClassifier()
RF.fit(X, y)
# Make a prediction
y_predict = RF.predict(X)
y_predict[:10]
(y == y_predict).mean()

